{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - 3D - 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from numpy.linalg import inv, pinv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the slides the steps from Algorithm 3:\n",
    "\n",
    "![title](algorithm_3.png)\n",
    "\n",
    "![title](PnP.png)\n",
    "\n",
    "# Exercise 1a)\n",
    "The steps 1)-2.1) has already been done, and is saved in corresponding files. The exercise is to implement step 2.2) by filling in the missing code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureTracking(prev_img, next_img, prev_points, world_points):\n",
    "    \"\"\"\n",
    "    Use OpenCV to find the prev_points from the prev_img in the next_img\n",
    "    Remember to remove points that could not be found from prev_points, next_points, and world_points\n",
    "    hint: status == 1\n",
    "    \"\"\"\n",
    "    params = dict(winSize=(21, 21),\n",
    "                 maxLevel=3,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "    \n",
    "    next_points, st, _ = cv2.calcOpticalFlowPyrLK(prev_img, next_img, prev_points, None, **params)\n",
    "\n",
    "    return world_points, prev_points, next_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Exercise 4 in week 2\n",
    "\n",
    "# Exercise 1b)\n",
    "Continue the algorithm by implementing step 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.] [-0.] [-5.58552547e-91] [4.45192829e+252] [7.20367614e+159] [8.46182488e-308]\n",
      "[-0.] [-0.] [-0.] [1.48976621e+161] [0.] [0.]\n",
      "[-0.] [-0.] [-1.38298973e-47] [0.] [0.] [0.]\n",
      "[-1.48976621e+161] [-0.] [-0.] [0.] [0.] [1.38298973e-47]\n",
      "[-0.] [-0.] [-9.99999465e-05] [0.] [0.] [0.]\n"
     ]
    }
   ],
   "source": [
    "K = np.array([[7.188560e+02, 0.000000e+00, 6.071928e+02], # camera matrix\n",
    "              [0, 7.188560e+02, 1.852157e+02],\n",
    "              [0, 0, 1]])\n",
    "\n",
    "reference_img = np.load(\"img_\" + str(0) + \".npy\")\n",
    "\n",
    "for t in range(1, 6):\n",
    "\n",
    "    # the image at current time=t\n",
    "    curImage = np.load(\"img_\" + str(t) + \".npy\")\n",
    "    # the 3D landmarks in the world coordinates which have been computed in time=t-1\n",
    "    landmark_3D = np.load(\"landmark_\" + str(t-1) + \".npy\")\n",
    "    # the 2D coordinates of the 3D points in the previous frame at time=t-1\n",
    "    reference_2D = np.load(\"reference_2D_\" + str(t-1) + \".npy\")\n",
    "    \n",
    "    # the 2D landmarks at the current time = t\n",
    "    landmark_3D, reference_2D, tracked_2Dpoints = featureTracking(reference_img, \n",
    "                                                                  curImage, \n",
    "                                                                  reference_2D,\n",
    "                                                                  landmark_3D)\n",
    "    \n",
    "    \"\"\"\n",
    "    Using OpenCV, implement PnP using Ransac\n",
    "    \"\"\"\n",
    "    #_, rvec, tvec, inliers = ...\n",
    "    _, rvec, tvec, inliers = cv2.solvePnPRansac(landmark_3D, tracked_2Dpoints, K, np.empty(4))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Transform the translation and rotation into the world frame\n",
    "    \"\"\"\n",
    "    tvec = -tvec\n",
    "    \n",
    "    print(tvec[0], tvec[1], tvec[2], rvec[0], rvec[1], rvec[2])\n",
    "\n",
    "    # update for next timestep\n",
    "    reference_img = curImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The output should look similar to:\n",
    "\n",
    "[-0.00110282] [-0.00067164] [-0.00078343] [-7.40069212e-05] [-7.35119065e-05] [9.84544279e-05]\n",
    "\n",
    "[-0.00363946] [-0.00875075] [0.67580842] [-0.0021666] [0.00325853] [-0.00244333]\n",
    "\n",
    "[-0.01096271] [-0.01635663] [1.3774094] [-0.00364615] [0.0075151] [-0.00099691]\n",
    "\n",
    "[-0.0315663] [-0.02560111] [2.0996797] [-0.00509583] [0.01121646] [-0.00082978]\n",
    "\n",
    "[-0.04971858] [-0.03532535] [2.8330071] [-0.00561424] [0.0161333] [0.00041981]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1c)\n",
    "What approximate direction did the camera move in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:\n",
      "Camera Position (tvec): [-0.00000000e+00 -0.00000000e+00 -5.58552547e-91]\n",
      "Camera Rotation (rvec): [0. 0. 0.]\n",
      "Direction of Movement: [-0. -0. -1.]\n",
      "Time 2:\n",
      "Camera Position (tvec): [-0.00000000e+00 -0.00000000e+00 -2.75478773e+15]\n",
      "Camera Rotation (rvec): [0.00000000e+00 0.00000000e+00 5.58552547e-91]\n",
      "Direction of Movement: [ 0.  0. -1.]\n",
      "Time 3:\n",
      "Camera Position (tvec): [-0. -0. -0.]\n",
      "Camera Rotation (rvec): [0.         0.         0.77558136]\n",
      "Direction of Movement: [0. 0. 1.]\n",
      "Time 4:\n",
      "Camera Position (tvec): [-0.         -0.          0.68234444]\n",
      "Camera Rotation (rvec): [0.00000000e+00 0.00000000e+00 2.75478773e+15]\n",
      "Direction of Movement: [0. 0. 1.]\n",
      "Time 5:\n",
      "Camera Position (tvec): [-1.48976621e+161 -0.00000000e+000 -0.00000000e+000]\n",
      "Camera Rotation (rvec): [0. 0. 0.]\n",
      "Direction of Movement: [-0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "# Camera intrinsic matrix\n",
    "K = np.array([[7.188560e+02, 0.000000e+00, 6.071928e+02],  # Camera matrix\n",
    "              [0, 7.188560e+02, 1.852157e+02],\n",
    "              [0, 0, 1]])\n",
    "\n",
    "reference_img = np.load(\"img_\" + str(0) + \".npy\")\n",
    "previous_tvec = np.zeros((3, 1))  # Initialize previous translation vector as zeros\n",
    "\n",
    "for t in range(1, 6):\n",
    "\n",
    "    # The image at current time=t\n",
    "    curImage = np.load(\"img_\" + str(t) + \".npy\")\n",
    "    # The 3D landmarks in the world coordinates which have been computed in time=t-1\n",
    "    landmark_3D = np.load(\"landmark_\" + str(t-1) + \".npy\")\n",
    "    # The 2D coordinates of the 3D points in the previous frame at time=t-1\n",
    "    reference_2D = np.load(\"reference_2D_\" + str(t-1) + \".npy\")\n",
    "    \n",
    "    # Track the 2D landmarks in the current image\n",
    "    landmark_3D, reference_2D, tracked_2Dpoints = featureTracking(reference_img, \n",
    "                                                                  curImage, \n",
    "                                                                  reference_2D,\n",
    "                                                                  landmark_3D)\n",
    "    \n",
    "    # Use OpenCV to solve PnP and get the camera pose\n",
    "    _, rvec, tvec, inliers = cv2.solvePnPRansac(landmark_3D, tracked_2Dpoints, K, np.empty(4))\n",
    "\n",
    "    # Transform the translation and rotation into the world frame\n",
    "    tvec = -tvec  # To get the camera position in the world coordinate system\n",
    "    \n",
    "    # Calculate the change in position (movement) by comparing with the previous frame's tvec\n",
    "    movement_vector = tvec - previous_tvec  # Movement between current and previous frame\n",
    "    \n",
    "    # Normalize the movement vector to get the approximate direction\n",
    "    direction = movement_vector / np.linalg.norm(movement_vector)\n",
    "    \n",
    "    # Print the camera's position, rotation (rvec), and direction of movement\n",
    "    print(f\"Time {t}:\")\n",
    "    print(f\"Camera Position (tvec): {tvec.flatten()}\")\n",
    "    print(f\"Camera Rotation (rvec): {rvec.flatten()}\")\n",
    "    print(f\"Direction of Movement: {direction.flatten()}\")\n",
    "    \n",
    "    # Update the previous tvec for the next iteration\n",
    "    previous_tvec = tvec\n",
    "\n",
    "    # Update reference image for next frame\n",
    "    reference_img = curImage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
